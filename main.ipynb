{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63318e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6df90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing train.bin: 100%|██████████| 1024/1024 [17:38<00:00,  1.03s/it]\n",
      "writing validation.bin: 100%|██████████| 1024/1024 [00:15<00:00, 66.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process(example):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcde78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        \n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f3b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e33aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     \n",
    "    block_size=128,       \n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d947309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086de730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d5591f0750>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4 \n",
    "max_iters = 20000\n",
    "warmup_steps = 1000 \n",
    "min_lr = 5e-4 \n",
    "eval_iters = 500 \n",
    "batch_size = 32 \n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' \n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d8e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_5180\\3302513730.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) \n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) \n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) \n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16eae6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]C:\\Users\\acer\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "  2%|▎         | 500/20000 [08:06<5:15:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 9.3905, val loss 9.4016\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 999/20000 [22:10<5:04:02,  1.04it/s]   C:\\Users\\acer\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "  5%|▌         | 1000/20000 [22:11<5:04:36,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 8.4392, val loss 8.4427\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1500/20000 [36:14<4:59:11,  1.03it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500: train loss 7.4938, val loss 7.4970\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2000/20000 [50:17<4:49:33,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: train loss 6.6514, val loss 6.6498\n",
      "The current learning rate: 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2500/20000 [1:04:18<4:31:50,  1.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500: train loss 5.9649, val loss 5.9648\n",
      "The current learning rate: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3000/20000 [1:18:16<4:32:41,  1.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000: train loss 5.4633, val loss 5.4614\n",
      "The current learning rate: 0.00011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3500/20000 [1:32:13<4:24:46,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3500: train loss 5.0532, val loss 5.0481\n",
      "The current learning rate: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4000/20000 [1:46:11<5:30:25,  1.24s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000: train loss 4.7330, val loss 4.7347\n",
      "The current learning rate: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 4500/20000 [2:00:07<4:08:33,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4500: train loss 4.4935, val loss 4.4984\n",
      "The current learning rate: 0.00013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5000/20000 [2:14:04<3:59:48,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000: train loss 4.2870, val loss 4.2828\n",
      "The current learning rate: 0.00014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5500/20000 [2:27:58<3:51:35,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5500: train loss 4.1352, val loss 4.1420\n",
      "The current learning rate: 0.00015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6000/20000 [2:41:53<3:42:42,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000: train loss 3.9777, val loss 3.9799\n",
      "The current learning rate: 0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 6500/20000 [2:55:49<3:29:48,  1.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6500: train loss 3.8378, val loss 3.8387\n",
      "The current learning rate: 0.00018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7000/20000 [3:09:49<3:30:14,  1.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000: train loss 3.7160, val loss 3.7170\n",
      "The current learning rate: 0.00019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 7500/20000 [3:23:54<3:20:01,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7500: train loss 3.6068, val loss 3.6102\n",
      "The current learning rate: 0.00020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8000/20000 [3:38:04<4:09:32,  1.25s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8000: train loss 3.4979, val loss 3.5042\n",
      "The current learning rate: 0.00022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 8500/20000 [3:51:58<3:02:54,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8500: train loss 3.4174, val loss 3.4247\n",
      "The current learning rate: 0.00024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9000/20000 [4:05:53<2:54:36,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000: train loss 3.3246, val loss 3.3263\n",
      "The current learning rate: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 9500/20000 [4:20:03<2:47:43,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9500: train loss 3.2583, val loss 3.2573\n",
      "The current learning rate: 0.00027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10000/20000 [4:33:54<2:39:31,  1.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000: train loss 3.1782, val loss 3.1807\n",
      "The current learning rate: 0.00028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 10500/20000 [4:47:59<2:30:24,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10500: train loss 3.1105, val loss 3.1174\n",
      "The current learning rate: 0.00030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11000/20000 [5:02:15<2:24:42,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11000: train loss 3.0577, val loss 3.0632\n",
      "The current learning rate: 0.00032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 11500/20000 [5:16:21<2:16:07,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11500: train loss 2.9902, val loss 2.9946\n",
      "The current learning rate: 0.00033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12000/20000 [5:30:25<2:47:41,  1.26s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12000: train loss 2.9357, val loss 2.9414\n",
      "The current learning rate: 0.00035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 12500/20000 [5:44:27<1:58:35,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12500: train loss 2.8842, val loss 2.8897\n",
      "The current learning rate: 0.00036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13000/20000 [5:58:15<1:50:55,  1.05it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13000: train loss 2.8354, val loss 2.8444\n",
      "The current learning rate: 0.00038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13500/20000 [6:12:04<1:43:55,  1.04it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13500: train loss 2.7988, val loss 2.8045\n",
      "The current learning rate: 0.00040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14000/20000 [6:26:09<1:38:32,  1.01it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14000: train loss 2.7591, val loss 2.7640\n",
      "The current learning rate: 0.00041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 14500/20000 [6:40:01<1:24:46,  1.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14500: train loss 2.7160, val loss 2.7175\n",
      "The current learning rate: 0.00042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15000/20000 [6:53:54<1:19:24,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15000: train loss 2.6829, val loss 2.6828\n",
      "The current learning rate: 0.00044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 15500/20000 [7:06:23<1:01:21,  1.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15500: train loss 2.6445, val loss 2.6403\n",
      "The current learning rate: 0.00045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16000/20000 [7:19:06<1:21:10,  1.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16000: train loss 2.5948, val loss 2.6031\n",
      "The current learning rate: 0.00046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 16500/20000 [7:31:44<48:02,  1.21it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16500: train loss 2.5692, val loss 2.5778\n",
      "The current learning rate: 0.00047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17000/20000 [7:44:34<43:08,  1.16it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17000: train loss 2.5332, val loss 2.5343\n",
      "The current learning rate: 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 17500/20000 [7:57:33<36:22,  1.15it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17500: train loss 2.5016, val loss 2.5039\n",
      "The current learning rate: 0.00048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18000/20000 [8:10:03<30:41,  1.09it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18000: train loss 2.4742, val loss 2.4838\n",
      "The current learning rate: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 18500/20000 [8:22:28<20:20,  1.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18500: train loss 2.4540, val loss 2.4586\n",
      "The current learning rate: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19000/20000 [8:34:55<13:41,  1.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19000: train loss 2.4150, val loss 2.4259\n",
      "The current learning rate: 0.00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 19500/20000 [8:47:00<07:05,  1.18it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19500: train loss 2.3963, val loss 2.4005\n",
      "The current learning rate: 0.00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [9:00:03<00:00,  1.62s/it]    \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0474cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5102b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_5180\\2462269081.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(config)  \n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cce8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin.\n",
      "\n",
      "\"Hello weighed sweet similar day,\" the creature said.\n",
      "\n",
      "Kate looked up to the bee. It was so soft and powerful. She was happy!\n",
      "\n",
      "The gorilla found a whole place with lots of things - from the other types of new stuff. Ruff inspired teddy bear had been full of bones and was able to play again.\n",
      "\n",
      "The queen was content asGreat.\n",
      "\n",
      "Kelly was amazed and had made the best risks, and she took well and smiled. The pig rubbing the power from the wall to do worrying.Once upon a time, there was a little boy named Jack. Jack loved to observe things his dream. Every morning, he would mix things. One day, his mother took him on something purple and wore a sparkly pattern in aped one by himself as a landscape.\n",
      "\n",
      "He showed everyone all about it and everyone was very glad. It was a Nurse that soon. Jake was so excited to see it again, so the cabin was\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e02aabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a girl. She was three years old. \n",
      "\n",
      "One day, she had a special song about taking her book from her room class. She was so excited about the pages. \n",
      "\n",
      "One day, she packed all the parts as she came in her computer. She went to the achieve her room and put it on her own and started to play with her bright jewelry. \n",
      "\n",
      "Millie's goal fit perfectly blinking loud, and hard work herselfrings was safe. But, she started to work too far away. She was so frightened and grabbed her ticket and ran back to her past her clothes and tried to come out, but was an amazing staff in the corner. \n",
      "\n",
      "At the end of the class, Paul put her hammer everywhere skills filled it and quickly open them zoomed out of the window. All could peeked out to the hut and over to see where there was a toy so many cane, and then he slowlyPokemon. It was herummy umbrellaened up as\n"
     ]
    }
   ],
   "source": [
    "sentence = \"There was a girl\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb553a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can he get the job? \n",
      "\n",
      "Jackty smiled and said, \"Why do you want to sell me something to me?\".\n",
      "\n",
      "Jack said, \"Yes, we can buy lots of food!\"Once upon a time there was a girl named Sarah. She was three years old and she loved the years old a very much. Every Wednesday Sarah would come to choose something special with her collection. \n",
      "\n",
      "Every day, Sarah would paint all by exploring the wild forest and people in the park. One day one day she noticed something in the park. Then she started to paint and draw in circles, just as importantlyly as she went. She's brother asked her if she could have better look. It was a special place to be seen. Sarah was so excited to do a celebration to see appeared on the scale and how she was wrapping out around the cool build.\n",
      "\n",
      "The next day, Sarah took out his pictures and her some toast roles and put them at the lock. It was a harmless place to\n"
     ]
    }
   ],
   "source": [
    "sentence = \"can he get the job?\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554ed40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
